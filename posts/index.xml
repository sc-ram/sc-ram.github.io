<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Welcome to sc-ram</title><link>https://sc-ram.github.io/posts/</link><description>Recent content in Posts on Welcome to sc-ram</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sun, 02 May 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://sc-ram.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Backup data from single Synology NAS drive</title><link>https://sc-ram.github.io/posts/20210502_synology_nas_drive_backup/</link><pubDate>Sun, 02 May 2021 00:00:00 +0000</pubDate><guid>https://sc-ram.github.io/posts/20210502_synology_nas_drive_backup/</guid><description>What happens if your Synology is gone but the drive is still there?
I recently upgraded my 2-bay Synology NAS with bigger drives by replacing one drive after the other with a bigger one and running a repair step afterwards. That worked perfectly fine and did not take too long. However, afterwards I wanted too see if I could retrieve data from the drives even though they had been setup using SHR (Synology Hybrid Raid).</description><content type="html"><![CDATA[<p>What happens if your Synology is gone but the drive is still there?</p>
<p>I recently upgraded my 2-bay Synology NAS with bigger drives by replacing one drive after the other with a bigger one
and running a repair step afterwards. That worked perfectly fine and did not take too long. However, afterwards I wanted
too see if I could retrieve data from the drives even though they had been setup using SHR (Synology Hybrid Raid).
This could come handy if the NAS dies but the drives are still intact. As I am working on my old drives there is no risk
of losing data. But playing with the raid configuration can of course lead to invalid raid configurations and data loss.</p>
<p>So let&rsquo;s see how far we can get&hellip;</p>
<h2 id="connect-drive-usb-or-sata-and-check-device-name">Connect drive (USB or SATA) and check device name</h2>
<pre><code>$ lsblk
...

sdc               8:32   0   1.8T  0 disk 
├─sdc1            8:33   0   2.4G  0 part 
├─sdc2            8:34   0     2G  0 part 
├─sdc3            8:35   0     1K  0 part 
└─sdc5            8:37   0   1.8T  0 part
</code></pre><p>I don&rsquo;t have too many drives attached to my machine so finding the correct block device is easy. In my case it is sdc.
Apparently it contains 4 partitions. Let&rsquo;s see what kind of raid SHR really is.</p>
<h2 id="check-raid-configuration">Check Raid configuration</h2>
<pre><code>$ sudo mdadm --examine /dev/sdc5
/dev/sdc5:
          Magic : a92b4efc
        Version : 1.2
    Feature Map : 0x0
     Array UUID : b424cd2c:14d42b7d:b24e6e57:48e95d1b
           Name : DiskStation:2
  Creation Time : Fri Feb  3 10:09:23 2017
     Raid Level : raid1
   Raid Devices : 2

 Avail Dev Size : 3897366912 (1858.41 GiB 1995.45 GB)
     Array Size : 1948683456 (1858.41 GiB 1995.45 GB)
    Data Offset : 2048 sectors
   Super Offset : 8 sectors
   Unused Space : before=1968 sectors, after=0 sectors
          State : clean
    Device UUID : 3a6825ba:a1548281:c02a8604:47837a28

    Update Time : Wed Apr 21 19:54:13 2021
       Checksum : d1cd24ec - correct
         Events : 446


   Device Role : Active device 1
   Array State : AA ('A' == active, '.' == missing, 'R' == replacing)
</code></pre><p>Apparently we can work with raid 1 after all. Let&rsquo;s see if we can create a running raid configuration and mount it.</p>
<h2 id="assemble-raid-from-single-disk">Assemble raid from single disk</h2>
<pre><code>$ sudo mdadm -A -R /dev/md3 /dev/sdc5
mdadm: Merging with already-assembled /dev/md/DiskStation:2
mdadm: failed to add /dev/sdc5 to /dev/md/DiskStation:2: Device or resource busy
mdadm: /dev/md/DiskStation:2 has been started with 0 drives (out of 2).
</code></pre><p>Apparently mdadm already created a raid. As I want to set it up manually I will stop this raid for now.
Especially as it will not work with 0 drives.</p>
<pre><code>$ sudo mdadm --stop /dev/md/DiskStation\:2 
mdadm: stopped /dev/md/DiskStation:2
</code></pre><p>Let&rsquo;s try the assemble step once again</p>
<pre><code>$ sudo mdadm -A -R /dev/md3 /dev/sdc5
mdadm: /dev/md3 has been started with 1 drive (out of 2).
</code></pre><p>That looks much better. This time one drive has been started which is enough for raid 1. Let&rsquo;s try to mount it.</p>
<h2 id="lets-try-to-mount-it---part1">Let&rsquo;s try to mount it - part1</h2>
<pre><code>$ sudo mkdir /mnt/syno
</code></pre><p>First create a directory to mount the device.</p>
<pre><code>$ sudo mount /dev/md3 /mnt/syno/
mount: /mnt/syno: unknown filesystem type 'LVM2_member'.
</code></pre><p>Not so fast! Synology seems to use LVM on top of the raid to slice the volumes. So more work is required.</p>
<h2 id="lvm-setup-required">LVM setup required</h2>
<pre><code>$ sudo vgscan
WARNING: PV /dev/md3 in VG vg1000 is using an old PV header, modify the VG to update.
Found volume group &quot;vg1000&quot; using metadata type lvm2
</code></pre><p>OK so there is a volume group <code>vg1000</code> on the device we just set up using mdadm.
Let&rsquo;s see what LVM can tell us about the volume.</p>
<pre><code>$ sudo lvdisplay
  WARNING: PV /dev/md3 in VG vg1000 is using an old PV header, modify the VG to update.
  --- Logical volume ---
  LV Path                /dev/vg1000/lv
  LV Name                lv
  VG Name                vg1000
  LV UUID                2gf4Pt-Zn61-CMXW-fBBu-ybWH-gbCi-pqe3q3
  LV Write Access        read/write
  LV Creation host, time , 
  LV Status              available
  # open                 0
  LV Size                1.81 TiB
  Current LE             475752
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     131064
  Block device           253:3
</code></pre><p>This is interesting as it already tells us the LV Path as <code>/dev/vg1000/lv</code>.
So let&rsquo;s try to mount it once again.</p>
<h2 id="lets-try-to-mount-it---part2">Let&rsquo;s try to mount it - part2</h2>
<pre><code>$ sudo mount /dev/vg1000/lv /mnt/syno/
</code></pre><p>There is no error. Does that mean we managed to mount it finally?</p>
<pre><code>$ cd /mnt/syno
</code></pre><pre><code>$ ls
Backup   Data   Music
</code></pre><p>Well, I guess we managed to get to the data.
The drive is mounted correctly and we can access the data on the drive.
It contains just the same data like Volume1 on the Synology UI.
After all it is just a LVM volume.</p>
<h2 id="learnings">Learnings</h2>
<ul>
<li>Synology uses software raid based on mdadm for raid setup</li>
<li>Volumes are sliced using LVM on top of the raid devices</li>
<li>Now I really want to do that with a 4-bay NAS to see if there are any differences. Unfortunately I don&rsquo;t have one.</li>
</ul>
]]></content></item><item><title>GitHub Actions FTW</title><link>https://sc-ram.github.io/posts/20210321_gh_actions/</link><pubDate>Sun, 21 Mar 2021 00:00:00 +0000</pubDate><guid>https://sc-ram.github.io/posts/20210321_gh_actions/</guid><description>Using GitHub Actions with Hugo to build this site
Making use of GitHub actions in combination with Hugo is a nice and easy way to build static web pages. The following resources helped to make this really easy:
GH actions by peaceiris Hermit Template Nice blog as baseline</description><content type="html"><![CDATA[<p>Using GitHub Actions with Hugo to build this site</p>
<p>Making use of GitHub actions in combination with Hugo is a nice and easy way to build static web pages.
The following resources helped to make this really easy:</p>
<ul>
<li><a href="https://github.com/peaceiris/actions-gh-pages">GH actions by peaceiris</a></li>
<li><a href="https://github.com/Track3/hermit">Hermit Template</a></li>
<li><a href="https://blog.kye.dev/hugo-github-pages-actions/">Nice blog as baseline</a></li>
</ul>
]]></content></item></channel></rss>